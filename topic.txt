nano ~/.kube/config
kubectl config view
kubectl cluster-info
kubectl config current-context
kubectl config get-contexts
kubectl config use-context gke_${DEVSHELL_PROJECT_ID}_us-central1_autopilot-cluster-1

Pods Commands
kubectl exec -it new-nginx -- /bin/bash
kubectl port-forward new-nginx 10081:80
curl http://127.0.0.1:10081/test.html
source <(kubectl completion bash)
kubectl cp ~/test.html $my_nginx_pod:/usr/share/nginx/html/test.html


https://docs.honeycomb.io/manage-data-volume/refinery/set-up/


sudo apt-get update
sudo apt-get install apt-transport-https gnupg lsb-release
curl https://baltocdn.com/helm/signing.asc | sudo apt-key add -
echo "deb https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm


gcloud config set project corp-test-mgmt-anthos-3578
gcloud container fleet memberships get-credentials non-prod-ecommerce-internal
helm repo add honeycomb https://honeycombio.github.io/helm-charts
helm repo update refinery honeycomb/refinery
helm repo update refinery honeycomb/refinery
helm repo update refinery honeycomb
helm repo update honeycomb
helm repo update
helm list -A
helm search repo honeycomb
helm install honeycomb-refinery honeycomb/refinery --namespace honeycomb
kubectl create namespace honeycomb
kubectl get ns
helm install refinery honeycomb/refinery --namespace honeycomb
helm uninstall honeycomb-refinery --namespace honeycomb
helm list -A

Good morning, Steven. I might not be able to make the standup today because my internet connection will be down for some repairs, due to issues caused by the recent wind. However, my tickets are up to date, and Shaun is also aware of what I am working on. Here are the links to my tickets:

helm install harbor harbor/harbor -n harbor -f my-harbor-values.yaml


Subject: ðŸš¨ Urgent: [App Name] on [Cluster Name] Error Alert ðŸš¨

Hi Team,

ðŸ”” Alert from PagerDuty: We've detected an issue with [App Name] on [Cluster Name]. ðŸ› ï¸ Can someone investigate this ASAP?

Thanks,


# Download NGINX from Docker Hub
docker pull nginx

# Tag the NGINX image for your Harbor registry
docker tag nginx <your-harbor-registry>/te-core-harbor-xxxx.com/nginx

# Log in to your Harbor registry
docker login <your-harbor-registry> -u admin -p <your-harbor-admin-password>

# Push the tagged NGINX image to your Harbor registry
docker push <your-harbor-registry>/te-core-harbor-xxxx.com/nginx

# Deploy NGINX from your Harbor registry into your GCP cluster
kubectl create deployment nginx-deployment --image=<your-harbor-registry>/te-core-harbor-xxxx.com/nginx

# Expose NGINX using a LoadBalancer service
kubectl expose deployment nginx-deployment --name=nginx-service --type=LoadBalancer --port=80 --target-port=80

# Wait for the external IP to be provisioned
kubectl get svc nginx-service

# Access NGINX using the external IP
{
  "builder": {
    "gc": {
      "defaultKeepStorage": "20GB",
      "enabled": true
    }
  },
  "experimental": false,
  "features": {
    "buildkit": true
  },
  "insecure-registries": [
    "te-core-harbor-anthos.sportski.com"
  ]
}


resource "aws_sns_topic_subscription" "trusted_ledger_sqs_target" {
  topic_arn = "arn:aws:sns:us-east-1:${var.deploy_environment_id}:${local.trust_score_context}payments-user-trust-score-trusted-sns"
  protocol  = "sqs"
  endpoint  = module.sqs.sqs_topic_arn
}





# Variable Definitions

# Deployment environment ID - used to identify different environments like dev, staging, or prod
variable "deploy_environment_id" {
  description = "Deployment environment ID"
  type        = string
}

# Local Values

# Local variables for easy management and readability
locals {
  # Trust score context - a specific identifier related to the trust score functionality
  trust_score_context = "your_trust_score_context" # Replace with your actual context

  # Constructing the SNS topic ARN dynamically based on the environment and context
  sns_topic_arn = "arn:aws:sns:us-east-1:${var.deploy_environment_id}:${local.trust_score_context}payments-user-trust-score-trusted-sns"

  # Determine if the SNS topic exists using the 'can' function
  sns_topic_exists = can(data.aws_sns_topic.existing_trusted_ledger_sns.arn)
}

# Data Sources

# Attempt to fetch the existing SNS topic using the constructed ARN
data "aws_sns_topic" "existing_trusted_ledger_sns" {
  arn = local.sns_topic_arn
}

# SQS Module/Resource (Placeholder)

# Ensure this module is defined in your Terraform or replace with actual SQS ARN
# This module should output the ARN of the SQS topic that will subscribe to the SNS topic
module "sqs" {
  # ... your SQS module configuration ...
  # sqs_topic_arn should be an output from this module
}

# Resources

# Conditionally create the SNS topic subscription only if the SNS topic exists
resource "aws_sns_topic_subscription" "trusted_ledger_sqs_target" {
  count     = local.sns_topic_exists ? 1 : 0  # Create if SNS topic exists, else do not create
  topic_arn = data.aws_sns_topic.existing_trusted_ledger_sns.arn  # ARN of the existing SNS topic
  protocol  = "sqs"  # Protocol for the subscription, here it's SQS
  endpoint  = module.sqs.sqs_topic_arn  # Endpoint ARN for the SQS queue
}


Variables would be my go-to.





13:28
Also bear in mind that if youâ€™re using counts/for_Each loops, youâ€™re ultimately changing the resources, so theyâ€™ll want to recreate
13:30
So, something like aws_sns_topic.blahblah will ultimately become something like aws_sns_topic.blahblakh[0] if using a count, and aws_sns_topic.blahblah["blahblah_from_for_each"] for for_each loops.
13:30
Youâ€™re ultimately going to have to use terraform move blocks as well if thatâ€™s the case.
